# Feature Analysis Template

## Feature Information

- **Feature Name:** [Feature title]
- **Feature ID:** [ID from feature.json]
- **Phase:** [1-7]
- **Execution Start:** [Timestamp]
- **Execution End:** [Timestamp]
- **Duration:** [Minutes]

---

## Agent Invocation Tracking

| Agent                             | Invoked?   | Contribution Type               | Unique Value? |
| --------------------------------- | ---------- | ------------------------------- | ------------- |
| **Sage** (Business/Product)       | ☐ Yes ☐ No | [Analysis/Strategy/Product]     | ☐ Yes ☐ No    |
| **Theo** (Technical Architecture) | ☐ Yes ☐ No | [Architecture/Design/Tech]      | ☐ Yes ☐ No    |
| **Finn** (Financial Strategy)     | ☐ Yes ☐ No | [Cost/ROI/Financial]            | ☐ Yes ☐ No    |
| **Cerberus** (Security)           | ☐ Yes ☐ No | [Security/Privacy/Compliance]   | ☐ Yes ☐ No    |
| **Mary** (Marketing/GTM)          | ☐ Yes ☐ No | [Marketing/Messaging/GTM]       | ☐ Yes ☐ No    |
| **Walt** (Operations)             | ☐ Yes ☐ No | [Operations/Process/Efficiency] | ☐ Yes ☐ No    |
| **Axel** (UX/Design)              | ☐ Yes ☐ No | [UX/UI/Design]                  | ☐ Yes ☐ No    |
| **Apex** (Strategic Oversight)    | ☐ Yes ☐ No | [Strategy/Vision/Alignment]     | ☐ Yes ☐ No    |
| **Zen** (Quality/Risk)            | ☐ Yes ☐ No | [Quality/Risk/Validation]       | ☐ Yes ☐ No    |
| **Echon** (Implementation)        | ☐ Yes ☐ No | [Implementation/Delivery]       | ☐ Yes ☐ No    |

**Agents Invoked:** [X]/10

---

## Per-Agent Contribution Detail

### 1. Sage (Business/Product Strategy)

**Expected for this feature:**

- [What business/product insights should Sage provide?]
- [What strategic questions should be addressed?]

**Actual contribution:**

- [What did Sage actually contribute?]
- [Key insights or recommendations]
- [Links to specific output sections]

**Uniqueness Score:** [0-3]

- 0 = Not invoked or generic
- 1 = Present but overlaps with others
- 2 = Distinct perspective with some overlap
- 3 = Unique, irreplaceable domain insight

---

### 2. Theo (Technical Architecture)

**Expected for this feature:**

- [What technical architecture decisions should Theo address?]
- [What scalability/performance considerations?]

**Actual contribution:**

- [What did Theo actually contribute?]
- [Key technical decisions or patterns]
- [Links to specific output sections]

**Uniqueness Score:** [0-3]

- 0 = Not invoked or generic
- 1 = Present but overlaps with others
- 2 = Distinct perspective with some overlap
- 3 = Unique, irreplaceable domain insight

---

### 3. Finn (Financial Strategy)

**Expected for this feature:**

- [What cost/pricing/ROI analysis should Finn provide?]
- [What financial implications should be considered?]

**Actual contribution:**

- [What did Finn actually contribute?]
- [Key financial insights or models]
- [Links to specific output sections]

**Uniqueness Score:** [0-3]

- 0 = Not invoked or generic
- 1 = Present but overlaps with others
- 2 = Distinct perspective with some overlap
- 3 = Unique, irreplaceable domain insight

---

### 4. Cerberus (Security Guardian)

**Expected for this feature:**

- [What security risks should Cerberus identify?]
- [What compliance/privacy considerations?]

**Actual contribution:**

- [What did Cerberus actually contribute?]
- [Key security recommendations or controls]
- [Links to specific output sections]

**Uniqueness Score:** [0-3]

- 0 = Not invoked or generic
- 1 = Present but overlaps with others
- 2 = Distinct perspective with some overlap
- 3 = Unique, irreplaceable domain insight

---

### 5. Mary (Marketing/GTM Strategy)

**Expected for this feature:**

- [What marketing/messaging should Mary provide?]
- [What go-to-market considerations?]

**Actual contribution:**

- [What did Mary actually contribute?]
- [Key marketing insights or campaigns]
- [Links to specific output sections]

**Uniqueness Score:** [0-3]

- 0 = Not invoked or generic
- 1 = Present but overlaps with others
- 2 = Distinct perspective with some overlap
- 3 = Unique, irreplaceable domain insight

---

### 6. Walt (Operations Commander)

**Expected for this feature:**

- [What operational processes should Walt define?]
- [What efficiency/scalability considerations?]

**Actual contribution:**

- [What did Walt actually contribute?]
- [Key operational processes or improvements]
- [Links to specific output sections]

**Uniqueness Score:** [0-3]

- 0 = Not invoked or generic
- 1 = Present but overlaps with others
- 2 = Distinct perspective with some overlap
- 3 = Unique, irreplaceable domain insight

---

### 7. Axel (UX/Design)

**Expected for this feature:**

- [What UX/design patterns should Axel recommend?]
- [What user experience considerations?]

**Actual contribution:**

- [What did Axel actually contribute?]
- [Key design decisions or patterns]
- [Links to specific output sections]

**Uniqueness Score:** [0-3]

- 0 = Not invoked or generic
- 1 = Present but overlaps with others
- 2 = Distinct perspective with some overlap
- 3 = Unique, irreplaceable domain insight

---

### 8. Apex (Strategic Oversight)

**Expected for this feature:**

- [What strategic alignment should Apex ensure?]
- [What cross-domain coordination is needed?]

**Actual contribution:**

- [What did Apex actually contribute?]
- [Key strategic guidance or coordination]
- [Links to specific output sections]

**Uniqueness Score:** [0-3]

- 0 = Not invoked or generic
- 1 = Present but overlaps with others
- 2 = Distinct perspective with some overlap
- 3 = Unique, irreplaceable domain insight

---

### 9. Zen (Quality/Risk Management)

**Expected for this feature:**

- [What quality gates should Zen define?]
- [What risks should be identified?]

**Actual contribution:**

- [What did Zen actually contribute?]
- [Key quality criteria or risk mitigations]
- [Links to specific output sections]

**Uniqueness Score:** [0-3]

- 0 = Not invoked or generic
- 1 = Present but overlaps with others
- 2 = Distinct perspective with some overlap
- 3 = Unique, irreplaceable domain insight

---

### 10. Echon (Implementation/Delivery)

**Expected for this feature:**

- [What implementation plan should Echon provide?]
- [What delivery strategy?]

**Actual contribution:**

- [What did Echon actually contribute?]
- [Key implementation decisions or tasks]
- [Links to specific output sections]

**Uniqueness Score:** [0-3]

- 0 = Not invoked or generic
- 1 = Present but overlaps with others
- 2 = Distinct perspective with some overlap
- 3 = Unique, irreplaceable domain insight

---

## Synthesis Analysis

### Did synthesis incorporate all 10 perspectives?

- ☐ **Yes** - All invoked agents clearly represented
- ☐ **Partial** - Some agent perspectives missing or diluted
- ☐ **No** - Synthesis ignored or minimized agent contributions

### Perspectives missing from synthesis:

- [ ] Sage (Business/Product)
- [ ] Theo (Technical Architecture)
- [ ] Finn (Financial Strategy)
- [ ] Cerberus (Security)
- [ ] Mary (Marketing/GTM)
- [ ] Walt (Operations)
- [ ] Axel (UX/Design)
- [ ] Apex (Strategic Oversight)
- [ ] Zen (Quality/Risk)
- [ ] Echon (Implementation)

### Missing perspective details:

[For each checked agent above, explain what perspective was missing and why it matters]

### Quality of synthesis:

- **Score:** [0-5]/5

**Scoring rubric:**

- 0 = No synthesis or single-perspective output
- 1 = Weak synthesis, mostly one or two perspectives
- 2 = Basic synthesis, some integration but gaps
- 3 = Good synthesis, most perspectives integrated
- 4 = Strong synthesis, all perspectives balanced
- 5 = Excellent synthesis, seamless multi-domain integration

**Synthesis strengths:**

- [What worked well in the synthesis?]

**Synthesis weaknesses:**

- [What could be improved?]

---

## Domain Coverage Analysis

### Business Domain (Sage, Finn, Mary)

**Invoked:** [X]/3 agents
**Unique Contributions:** [X]/9 points
**Coverage Quality:** ☐ Complete ☐ Partial ☐ Weak ☐ Missing

**Notes:**
[How well did business perspectives cover product, financial, and market aspects?]

---

### Technical Domain (Theo, Cerberus, Echon)

**Invoked:** [X]/3 agents
**Unique Contributions:** [X]/9 points
**Coverage Quality:** ☐ Complete ☐ Partial ☐ Weak ☐ Missing

**Notes:**
[How well did technical perspectives cover architecture, security, and implementation?]

---

### Experience Domain (Axel, Walt)

**Invoked:** [X]/2 agents
**Unique Contributions:** [X]/6 points
**Coverage Quality:** ☐ Complete ☐ Partial ☐ Weak ☐ Missing

**Notes:**
[How well did experience perspectives cover UX and operations?]

---

### Oversight Domain (Apex, Zen)

**Invoked:** [X]/2 agents
**Unique Contributions:** [X]/6 points
**Coverage Quality:** ☐ Complete ☐ Partial ☐ Weak ☐ Missing

**Notes:**
[How well did oversight perspectives cover strategy and quality/risk?]

---

## Feature Score Summary

| Metric                   | Score | Max    | Notes                                                     |
| ------------------------ | ----- | ------ | --------------------------------------------------------- |
| **Agents Invoked**       | [X]   | 10     | How many agents participated                              |
| **Unique Contributions** | [X]   | 30     | Sum of all uniqueness scores (10 agents × 3 max each)     |
| **Domain Coverage**      | [X]   | 10     | Business(3) + Technical(3) + Experience(2) + Oversight(2) |
| **Synthesis Quality**    | [X]   | 5      | How well perspectives were integrated                     |
| **TOTAL SCORE**          | [X]   | **55** | Overall feature quality                                   |

### Score Interpretation:

- **50-55:** Excellent - Full 10-agent engagement with unique, synthesized perspectives
- **40-49:** Good - Strong multi-agent collaboration with mostly unique contributions
- **30-39:** Fair - Moderate engagement but missing agents or lacking uniqueness
- **20-29:** Weak - Limited multi-agent value, significant gaps
- **0-19:** Poor - Single-agent or generic output, no real collaboration

---

## Key Findings

### Strengths:

1. [What worked exceptionally well?]
2. [Which agents provided the most value?]
3. [Where did synthesis excel?]

### Weaknesses:

1. [Which agents were missing or underutilized?]
2. [Where did contributions overlap without adding value?]
3. [Where did synthesis fail to integrate perspectives?]

### Recommendations:

1. [How can agent invocation be improved?]
2. [How can contribution uniqueness be enhanced?]
3. [How can synthesis quality be strengthened?]

---

## Evidence Links

**Feature File:** `.automaker/features/[feature-id]/feature.json`
**Agent Output:** `.automaker/features/[feature-id]/agent-output.md`
**Session Log:** `data/agent-sessions/[session-id].json`

**Key Output Sections:**

- [Agent 1 contribution]: Lines [X-Y] in agent-output.md
- [Agent 2 contribution]: Lines [X-Y] in agent-output.md
- [Synthesis section]: Lines [X-Y] in agent-output.md

---

## Reviewer Notes

**Reviewed by:** [Name]
**Review date:** [Date]
**Confidence in scoring:** ☐ High ☐ Medium ☐ Low

**Additional observations:**
[Any other insights or patterns noticed during analysis]
